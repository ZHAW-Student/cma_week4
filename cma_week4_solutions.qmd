---
title: "Exercise Week 4 solutions"
format: html
editor: visual
author: "Saskia Gianola"
message: false
warning: false
---

## Packages

```{r}
library("readr")
library("dplyr")
library("sf")
library("ggplot2")
library("tidyverse")
library("here")
library("XML")
library("lubridate")
library("ggmap")
library("geosphere")
library("SimilarityMeasures")
```

## Input

Load data and extract some movement of sabi. 

```{r}
wildschwein <- read_delim("wildschwein_BE_2056.csv", ",")
sabi <- wildschwein |>
  st_as_sf(coords = c("E", "N"), crs = 2056, remove = FALSE) |>
  filter(TierName == "Sabi", DatetimeUTC >= "2015-07-01", DatetimeUTC < "2015-07-03")
```

### Step a): Specify a temporal window

In the above dataset, the sampling interval is 15 minutes. If we take a temporal window of 60 minutes, that would mean including 4 fixes. We need to calculate the following Euclidean distances (pos representing single location):

pos[n-2] to pos[n]
pos[n-1] to pos[n]
pos[n] to pos[n+1]
pos[n] to pos[n+2]

### Step b): Measure the distance from every point to every other point within this temporal window

We can use the function distance_by_element from week 2 in combination with lead() and lag() to calculate the Euclidean distance. For example, to create the necessary offset of n-2, we use lag(x, 2). For each offset, we create one individual column.

```{r}
distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}

sabi <- sabi |>
    mutate(
        nMinus2 = distance_by_element(lag(geometry, 2), geometry),  # distance to pos -30 minutes
        nMinus1 = distance_by_element(lag(geometry, 1), geometry),  # distance to pos -15 minutes
        nPlus1  = distance_by_element(geometry, lead(geometry, 1)), # distance to pos +15 mintues
        nPlus2  = distance_by_element(geometry, lead(geometry, 2))  # distance to pos +30 minutes
    )
```
Now we want to calculate the mean distance of nMinus2, nMinus1, nPlus1, nPlus2 for each row. Since we want the mean value per Row, we have to explicitly specify this before mutate() with the function rowwise(). To remove this rowwise-grouping, we end the operation with ungroup().

Note that for the first two positions, we cannot calculate a stepMean since there is no Position n-2 for these positions. This is also true for the last to positions (lacking a position n+2).

```{r}
sabi <- sabi |>
    rowwise() |>
    mutate(
        stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
    ) |>
    ungroup()

sabi
```
### Step c): Remove “static points”

We can now determine if an animal is moving or not by specifying a threshold distance on stepMean. In our example, we use the mean value as a threshold: Positions with distances below this value are considered static.

```{r}
sabi <- sabi |>
    mutate(static = stepMean < mean(stepMean, na.rm = TRUE))

sabi_filter <- sabi |>
    filter(!static)

sabi_filter |>
    ggplot(aes(E, N)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "bottom")
```

# Task 1: Segmentation
Load and prepare data
```{r}
act1_parsed <- htmlTreeParse(file = "activities/11034695746.gpx", useInternalNodes = TRUE)
act1_parsed

# get coordinates
coords <- xpathSApply(doc = act1_parsed, path = "//trkpt", fun = xmlAttrs)

# get elevation
elevation <- xpathSApply(doc = act1_parsed, path = "//trkpt/ele", fun = xmlValue)

# get time
time <- xpathApply(doc = act1_parsed, path = "//trkpt/time", fun = xmlValue)


# built data frame
act1_df <- data.frame(
  lat = as.numeric(coords["lat", ]),
  lon = as.numeric(coords["lon", ]),
  ts_POSIXct = ymd_hms(time, tz = "UTC"),
  elevation = as.numeric(elevation)
)
act1_df

act1 <- act1_df |> 
  st_as_sf(coords = c("lat", "lon"), crs = 4326 , remove = FALSE)

```

Specify temporal window and measure distance in my data

```{r}
distance_by_element <- function(later, now) {
  as.numeric(
    st_distance(later, now, by_element = TRUE)
  )
}

act1 <- act1 |>
    mutate(
        nMinus2 = distance_by_element(lag(geometry, 2), geometry),  
        nMinus1 = distance_by_element(lag(geometry, 1), geometry),  
        nPlus1  = distance_by_element(geometry, lead(geometry, 1)), 
        nPlus2  = distance_by_element(geometry, lead(geometry, 2))  
    )

act1
```


# Task 2: Specify and apply threshold 
Calculate mean Step

```{r}
act1 <- act1 |>
    rowwise() |>
    mutate(
        stepMean = mean(c(nMinus2, nMinus1, nPlus1, nPlus2))
    ) |>
    ungroup()

act1
```
Explore mean step to define threashold

```{r}
hist(act1$stepMean)
boxplot(act1$stepMean)
summary(act1$stepMean)
```
The median is 2.94, so we take 2.9 as threashold

```{r}
act1 <- act1 |>
    mutate(static = stepMean < 2.9)
```


# Task 3 : Visualize segmented trajectories



```{r}
ggplot(data = act1, aes(lat, lon, col = static)) +
    geom_path() +
    geom_point() +
    coord_equal() +
    theme(legend.position = "bottom")

```

# Task 4: Segment-based analysis
Now we give each segment a unique ID

```{r}
rle_id <- function(vec) {
    x <- rle(vec)$lengths
    as.factor(rep(seq_along(x), times = x))
}

act1 <- act1 |>
    mutate(segment_id = rle_id(static))
```

Color by segement ID
```{r}
act1_filter <- act1 |>
    filter(!static)

act1_filter |>
    ggplot(aes(lat, lon, col = segment_id)) +
    geom_path() +
    geom_point() +
    coord_fixed() +
    theme(legend.position = "none")
```
Group by segment ID and remove shot segments

```{r}
act1 |> 
  group_by(segment_id)
```


# Task 5: Similarity measures
Now we work with a different dataset called pedestrians. We need to explore that dataset: 
```{r}
ped <- read_delim("pedestrian.csv", ",")
ped <- st_as_sf(ped, coords = c("E", "N"), crs = 2056, remove = FALSE)

later <- lag(ped$geometry)
now <- ped$geometry

ped$steplength <- distance_by_element(later, now)

difftime_secs <- function(later, now){
  as.numeric(difftime(later, now, units = "secs"))
}

now <- ped$DatetimeUTC
later <- lead(now)
ped$timelag_sec <- difftime_secs(later, now)
ped$speed <- ped$steplength/ped$timelag_sec

ped1 <- ped |> 
  filter(TrajID == 1)
ped2 <- ped |> 
  filter(TrajID == 2)
ped3 <- ped |> 
  filter(TrajID == 3)
ped4 <- ped |> 
  filter(TrajID == 4)
ped5 <- ped |> 
  filter(TrajID == 5)
ped6 <- ped |> 
  filter(TrajID == 6)

p1 <- ggplot(ped1, aes(E,N, col = speed)) +
  geom_point() +
  geom_path()
p2 <- ggplot(ped2, aes(E,N, col = speed)) +
  geom_point() +
  geom_path()
p3 <- ggplot(ped3, aes(E,N, col = speed)) +
  geom_point() +
  geom_path()
p4 <- ggplot(ped4, aes(E,N, col = speed)) +
  geom_point() +
  geom_path()
p5 <- ggplot(ped5, aes(E,N, col = speed)) +
  geom_point() +
  geom_path()
p6 <- ggplot(ped6, aes(E,N, col = speed)) +
  geom_point() +
  geom_path()


```
to plot all the six paths i used the function from http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/ 
```{r}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
multiplot(p1, p2, p3, p4, p5, p6, cols = 3)

```

# Task 6: Calculate similarity

I think that ID 4 is most dissimilar to others. I expect IDs 2 and 3 to be similar as well as 1 and 6 as they show similar paths and similar amount of points. ID 5 could be similar to 1 and 6 if the outliner is removed or does not have to much influence.    

As all similarity calculations need a matrix as an input, we need to extract the single trajectories from the dataset and save it as a matrix. 

```{r}
ped1_matrix <- ped1[,c(2,3)]
ped1_matrix <- st_drop_geometry(ped1_matrix)
ped1_matrix <- as.matrix(ped1_matrix)

ped2_matrix <- ped2[,c(2,3)]
ped2_matrix <- st_drop_geometry(ped2_matrix)
ped2_matrix <- as.matrix(ped2_matrix)

ped3_matrix <- ped3[,c(2,3)]
ped3_matrix <- st_drop_geometry(ped3_matrix)
ped3_matrix <- as.matrix(ped3_matrix)

ped4_matrix <- ped4[,c(2,3)]
ped4_matrix <- st_drop_geometry(ped4_matrix)
ped4_matrix <- as.matrix(ped4_matrix)

ped5_matrix <- ped5[,c(2,3)]
ped5_matrix <- st_drop_geometry(ped5_matrix)
ped5_matrix <- as.matrix(ped5_matrix)

ped6_matrix <- ped6[,c(2,3)]
ped6_matrix <- st_drop_geometry(ped6_matrix)
ped6_matrix <- as.matrix(ped6_matrix)

```

Now I can calculate similarity between trajectories with the newly created matrices. I start with the DTW.
```{r}
dtw1_2 <- DTW(ped1_matrix, ped2_matrix, pointSpacing = 2)
dtw1_3 <- DTW(ped1_matrix, ped3_matrix, pointSpacing = 2)
dtw1_4 <- DTW(ped1_matrix, ped4_matrix, pointSpacing = 2)
dtw1_5 <- DTW(ped1_matrix, ped5_matrix, pointSpacing = 2)
dtw1_6 <- DTW(ped1_matrix, ped6_matrix, pointSpacing = 2)

dtw1_2
dtw1_3 
dtw1_4
dtw1_5 
dtw1_6
```

The results from the DTW show that 1 is most similar to 4 and 6. 2 and 3 show very high values, whereas 5 has a value of -1. Now we take a look at the Edit Dist

```{r}
ed1_2 <- EditDist(ped1_matrix, ped2_matrix, pointDistance = 20)
ed1_3 <- EditDist(ped1_matrix, ped3_matrix, pointDistance = 20)
ed1_4 <- EditDist(ped1_matrix, ped4_matrix, pointDistance = 20)
ed1_5 <- EditDist(ped1_matrix, ped5_matrix, pointDistance = 20)
ed1_6 <- EditDist(ped1_matrix, ped6_matrix, pointDistance = 20)

ed1_2
ed1_3 
ed1_4
ed1_5 
ed1_6
```

This similarity measure shows that trajectory 1 is very similar to 5 and 6. 2, 3, and 4 are less similar. Next up is the Frechet.   
```{r}
fre1_2 <- Frechet(ped1_matrix, ped2_matrix, testLeash = -1)
fre1_3 <- Frechet(ped1_matrix, ped3_matrix, testLeash = -1)
fre1_4 <- Frechet(ped1_matrix, ped4_matrix, testLeash = -1)
fre1_5 <- Frechet(ped1_matrix, ped5_matrix, testLeash = -1)
fre1_6 <- Frechet(ped1_matrix, ped6_matrix, testLeash = -1)

fre1_2
fre1_3 
fre1_4
fre1_5 
fre1_6

```

The frechet tells us that 1 is similar to 2 and 6, whereas 3, 4 and 5 are less similar. Last we test the LCSS. 

```{r}
lcss1_2 <- LCSS(ped1_matrix, ped2_matrix, pointSpacing = 2, pointDistance = 20)
lcss1_3 <- LCSS(ped1_matrix, ped3_matrix, pointSpacing = 2, pointDistance = 20)
lcss1_4 <- LCSS(ped1_matrix, ped4_matrix, pointSpacing = 2, pointDistance = 20)
lcss1_5 <- LCSS(ped1_matrix, ped5_matrix, pointSpacing = 2, pointDistance = 20)
lcss1_6 <- LCSS(ped1_matrix, ped6_matrix, pointSpacing = 2, pointDistance = 20)

lcss1_2
lcss1_3 
lcss1_4
lcss1_5 
lcss1_6
```

This measure shows that 1 is most similar to 2 and 3, whereas 4, 5 and 6 are less similar.

Finally, I plot all the results to compare them. 

```{r}
similarity <- data.frame(
  dtw = c(dtw1_2, dtw1_3, dtw1_4, dtw1_5, dtw1_6),
  edit_dist = c(ed1_2, ed1_3, ed1_4, ed1_5, ed1_6),
  frechet = c(fre1_2, fre1_3, fre1_4, fre1_5, fre1_6),
  lcss = c(lcss1_2, lcss1_3, lcss1_4, lcss1_5, lcss1_6),
  traj = c("1_2","1_3","1_4","1_5","1_6")
)

plot_dtw <- ggplot(similarity, aes(x = traj, y = dtw)) +
  geom_bar(stat = "identity") +
  labs(
    title = "DTW"
  )

plot_ed <- ggplot(similarity, aes(x = traj, y = edit_dist)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Edit Dist"
  )


plot_fre <- ggplot(similarity, aes(x = traj, y = frechet)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Frechet"
  )

plot_lcss <- ggplot(similarity, aes(x = traj, y = lcss)) +
  geom_bar(stat = "identity") +
  labs(
    title = "LCSS"
  )

multiplot(plot_dtw, plot_ed, plot_fre, plot_lcss, cols = 2)

```

The results are similar to the ones presented on GitHub, the differences might be caused by different use of the parameters within the similariy measures (point distance etc.) Generally it shows that in order to get a conclusive statement about similarity of trajectories, a well-founded knowledge of the data and the similarity measures is needed. 